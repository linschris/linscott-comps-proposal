% !TEX TS-program = pdflatex
\documentclass[10pt,twocolumn]{article} 

% required packages for Oxy Comps style
\usepackage{oxycomps} % the main oxycomps style file
\usepackage{times} % use Times as the default font
\usepackage[style=numeric,sorting=nyt]{biblatex} % format the bibliography nicely 

\usepackage{amsfonts} % provides many math symbols/fonts
\usepackage{listings} % provides the lstlisting environment
\usepackage{amssymb} % provides many math symbols/fonts
\usepackage{graphicx} % allows insertion of grpahics
\usepackage{hyperref} % creates links within the page and to URLs
\usepackage{url} % formats URLs properly
\usepackage{verbatim} % provides the comment environment
\usepackage{xpatch} % used to patch \textcite
\usepackage{algpseudocode} % used for algorithm psuedocode
\usepackage{algorithm}

\graphicspath{{./images/}}

\bibliography{refs.bib}
\DeclareNameAlias{default}{last-first}

\xpatchbibmacro{textcite}
  {\printnames{labelname}}
  {\printnames{labelname} (\printfield{year})}
  {}
  {}

\pdfinfo{
    /Title (Image Search In Video Platforms With The Fuzzy C-Means Algorithm)
    /Author (Christopher Linscott)
}

\title{Image Search In Video Platforms With The Fuzzy C-Means Algorithm}

\author{Christopher Linscott}
\affiliation{Occidental College}
\email{clinscott@oxy.edu}

\begin{document}

\maketitle

% Refer to rubric: https://docs.google.com/document/d/1oiXngqxh30ADXVPfOEnNuBNX1DGFmmExI6DoGZNdrs0/edit

\section{Problem Context}

Often, when learning new topics or exploring new areas, one may come across a topic or object they’ve never seen before. Getting more information feels paradoxical, as obtaining further knowledge given only visual context is difficult; in a sense, you “don’t know what you don’t know”. In general, current video recommendation methods rely on textual information about the video using keywords \cite{Stanford2021}. Without knowing the name or a related keyword, one cannot simply search the web in hopes for an answer. Not only does image search help users tell the search engine what they’re looking for (without ambiguity), but image-related search removes the need to annotate images online with keywords as image similarity can be used as a heuristic \cite{Adrakatti2016}. \\
\indent The solution I propose involves utilizing video platforms like YouTube, as a means to solve this information gap using related videos, while also offering times within videos where the relevant content is found. A user would insert an image of an object in real life or even a drawing (from their lecture notes for example), and my project would return videos with related images either in the thumbnail or the video frames. If related video frames are found within the video, times corresponding to these frames will additionally be returned. \\
\indent YouTube videos can be broken down into images through both the thumbnail, an image preview of the video content, and the individual frames which make up the same video. With my project, we essentially create "baskets" of videos which relate to each other by visual characteristics (called “features”). Upon inserting an image, finding relevant information means finding the basket of images (that refer to videos or video frames) which relate to our input image by comparing features. By utilizing video frames as opposed to simply other images (like thumbnails), my project speeds up the searching process by returning a time within the video. From an educational perspective, not only does this help students fulfill these information gaps through videos, but it can speed up the studying process as users don’t always have to sift through a video to grab the relevant information.

\section{Technical Background} 

Creating baskets of related images and video frames requires using clustering (an unsupervised version of classification). To be more specific, clustering is the “method of segmenting a population into subgroups where members are more similar to each other than to members of other subgroups based on certain observed features” \cite{C3Clustering}. To be kurt, clustering is the methodology of creating subgroups of videos which relate to each other based on characteristics of one another. It’s referenced as unsupervised as the images and video frames we give will not have any pre-existing labels on them; finding datasets large enough, which includes these labels or categories, is a hard task in itself. Our population here will be a collection of images corresponding to objects in the real world, drawings, YouTube thumbnails, and YouTube video frames. Features, in this context, are visual characteristics of these images that may relate to other images. \\
\indent Fuzzy clustering, a type of clustering, says that a data point (i.e. an image) always resides uniquely inside a cluster (i.e. basket), but that it can reside in more than one cluster based on varying amounts of “belonging” \cite{PrasadClustering}. Each image would have a belongingness metric of some value between 0 and 1 to some number of clusters; you may also see this referenced as a “membership” metric. As images may have multiple components (including text, a background, people, and objects), this is more viable as opposed to a more strict clustering algorithm such as hierarchical clustering (where the baskets may be too small). Similar to k-means clustering, the number of clusters must be specified. For our use case, the specification of our clusters is normal, as we want to know how big or small these “baskets” may end up. Unlike k-means clustering, fuzzy clustering is preferred for images with lots of overlapping \cite{PrasadClustering}, which is exactly what should occur with these different types of images, as the images have related objects, shapes, and backgrounds. \\
\indent The algorithm used to perform Fuzzy clustering for this project will be the Fuzzy C-Means algorithm, which utilizes this function: \\
For a datapoint (image) I:
\[ I(k, m)  =  \sum_{j=1}^{k} \sum_{x_i \in C_j} u_{ij}^{m}(x_i - u_j)^2 \]
where \(u_{ij}\) is the degree of belongingness of a data point \(x_i \) to a cluster \(C_j \) which is between 0 and 1, \(u_j \) is the center of the cluster, and \(m \) is the fuzzifier (which converts strict into “fuzzy” sets with more blurred lines) \cite{PrasadClustering}. This function uses Euclidean distance, which is why \((x_i - u_j)^2 \) is present. The function takes in two parameters: \(k\), which represents the number of clusters and \(m \), the fuzzifier, which can be adjusted to make stricter or fuzzier clusters. The Fuzzy C-Means algorithm seeks to minimize the function and the Euclidean distance between the given data points and the center of the cluster. By minimizing the distance metric between a cluster’s center and its neighboring related data points, it makes baskets of datapoints (or images) which relate to each other visually, given that the computation of an image's visual characteristics fairly represents its features. It should be noted that we have freedom in how strict it makes these clusters, as a large \(m \) suppresses outliers in datasets, as the higher values of \(m \) allows more objects per cluster (i.e. sharing between clusters) \cite{Schwammle2010}. Whereas, with lower values, we allow less cluster “sharing” (i.e. less images to be partial members in many clusters). To determine the initial centroid (or initial place acting as the center of the cluster) as well as the membership values (overtime) we can solve for the following equations as done in Schwammle’s paper: 
\[u_j = \frac{\sum_{i=1}^Nu_{ij}^m x_i}{\sum_{i=1}^Nu_{ij}^m}\] 

\[u_j = \frac{1}{\sum_{s=1}^k \frac{(x_i - u_j)^2}{(x_i - u_s)^2} }\] 
By utilizing these equations, we can determine what clusters an image resides in (and how much it belongs to a given cluster), and recalculate the relative “baskets”. However, we have no way to get the initial data points. \\
\indent In order to determine information about an image, in processing images and assigning them a belongingness metric, we need to take into account the surroundings, noise, objects, and people in a given image. By assessing these visual characteristics, we can derive a datapoint for a particular image, and utilize the function and algorithms previously discussed. \\
\indent To do so, we can take inspiration from Kavitha’s fuzzy algorithm, utilized for the classification of satellite images, as follows: F is a satellite image, NB is the natural background, MM includes man-made objects, and NO describes noises \cite{Kavitha2020}.
\[F(i, j) = NB(i,j) + \sum_{x=1}^{n} MM(i, j) + NO(i, j) \]
\indent By repurposing this equation to account for an input image (or image in the dataset) to include objects and people, we can effectively begin to determine the data points for a given image. Utilizing this equation referenced above, we can utilize the Fuzzy C-Means algorithm as a means to cluster and determine how strongly a given image belongs to a given number of clusters.

\section{Prior Work}

Many image-related applications have been developed around gathering information. A popular application which has come out is Google Lens, an application which in real time can analyze an image (whether it has text or a given object) and identify the object or text. Furthermore, image-related search applications, such as Google’s reverse image search, are very useful in determining the source of an image or determining similar images based on an input image. In fact, it’s being used often for educational purposes such as with plant identification \cite{Moore2018}. In a similar way to how Google Lens and Google’s reverse image search aims to solve the information gap (given only visual context), my application serves to solve it using related videos as opposed to a purely trained AI model on the classification of objects. \\ 
\indent To go on, researchers at Stanford have utilized video frames within a given video (along with labeled good and bad thumbnails) to train a convolutional neural network to determine what frames of that video would constitute as a good “thumbnail”, or image preview to capture a user’s attention \cite{Stanford2017}. Extending this idea, we can utilize the video frames in order to gather images to act as “markers” for the videos; when an user-given image matches, it references the video and/or the time within it.\\
\indent Along with classifying YouTube thumbnails together, neural networks have also proven to be able to learn to cluster YouTube thumbnails together, categorizing them into different categories without the help of tags (i.e. labeled categories on the videos). Utilizing k-means clustering, they were able to compare and determine that a clustering algorithm could categorize videos by their thumbnails as well as YouTube did manually with tags \cite{Stanford2021}. So, with this paper in mind, as opposed to using classification like the first group of Stanford researchers, my project shifted toward using clustering because my datasets are not labeled and labeling data is very tedious. \\
\indent However, there are many ways (or algorithms) to cluster a dataset, depending on the type of data one is handling. Therefore, analyzing papers which tackled a similar problem led me to Kavitha’s paper, which retrieves images that are relevant to the user given image, for the purposes of getting information that’s useful for natural disaster management. The paper pushes that other image-retrieval algorithms are incomponent in terms of efficiency with clustering images, and that utilizing the Fuzzy characteristic algorithm can achieve great precision with better efficiency \cite{Kavitha2020}. Furthermore, an article about the many methods/algorithms of clustering agreed that Fuzzy algorithms are best for image segmentation which aligns with the arguments of the paper \cite{PrasadClustering}. As my project will involve deriving related images and image segmentation (in order to make the images more meaningful), my project landed on the Fuzzy C-Means Algorithm (as opposed to a K-means clustering) as the resource for clustering my images all together and creating these baskets.

\section {Methods}

\subsection {Approach}

From a high level standpoint, the approach involves taking an input image, running it through a convolutional neural network (CNN), grabbing the corresponding features derived from the CNN, and utilizing these as data points for the clustering algorithm, Fuzzy C Means (FCM). The clustering algorithm will take these data points and create baskets of videos based on similar features, thereby creating clusters of related videos which are similar to each other by visual characteristics. \\
\indent Clustering by the features of the convolutional neural network will result in much less noise and more meaningful generated “classes” or categories of the image clusters. If the raw images are small enough, we may be able to do it by individual pixels. However, possible noises present in image and video manipulation may interfere too heavily with this type of clustering. As well as, the number of features extracted by individual pixels may be far too many to be meaningful or considerable. Utilizing K-Means from sklearn (since K-Means and FCM are very similar in implementation and goal), I was able to cluster images by the raw image data of height h (pixels), width w (pixels), and 3 channels (red, green, and blue) as the images were colored. With images of distinctive features, the K-Means algorithm worked well when the object to cluster was alone (see Figure \ref{corgi:white}). However, the K-Means algorithm performed horribly when the same object was surrounded by other objects of meaning (such as tree) due to similar colors (see Figure \ref{corgi:tree}). Therefore, in the context of image manipulation and colored images, a convolutional neural network can help derive features from images such as edges, shape, position, distribution of colors, and edge orientation (source). \\
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2] {corgi_white}
  \vspace{20px}
  \caption{The K-Means clustering algorithm (f=2 for Fuzzy C Means) applied to an image of a Corgi in a white background.}
  \label{corgi:white}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2] {corgi_tree}
  \vspace{20px}
  \caption{The same clustering algorithm applied to an image of a corgi on a tree. Note how pixels (i.e. colors) of the tree and Corgi get grouped together.}
  \label{corgi:tree}
\end{figure}
\indent 
With this in mind, I want to utilize a CNN that takes an input image of dimensions (h, w, 3) and returns features {f1, …, fn} which will act as data points of dimension n for the Fuzzy C Means Algorithm. The convolutional neural network will have several convolutional layers which have 3 individual kernels underneath (for each of the different color channels red, green, and blue). These kernels are represented by a 3x3 matrix of either set values (Sobel filters), randomly initialized, or zero initialized values. The 2-D matrices are mapped over the image by computing a given value for a given set of 9 pixels for one channel. The values generated by this multiplication represent the activation (i.e. if it detects a given feature, such as an edge). However, an activation of 40 versus -10 can be seen as ambiguous. Therefore, upon the outputs of the kernels (called a feature map), they will be inserted into a ReLU activation function, which computes the activation (i.e. if the feature is recognized) based on if the value is positive (activates) or negative (doesn’t activate). After this convolutional layer and input through the ReLU function, the values are now simply activations or non activations. Therefore, we place these activations through a max pooling layer, which in short allows for “translational invariance”, or the idea that if the image is rotated, sized differently, or viewed in different lighting (source), that the object will still be recognized; this is necessary in the previously stated context of image manipulation. The max pooling layers will summarize the statistics of nearby locations, by also utilizing a filter of size 2x2 or 3x3. In short, the pooling layer will take the input from the previous layer and condense by selectively looking at a grid of pixels (the same size as the filter) and grabbing the maximum value of those pixels as its output. By reducing the size of input entering the next layer, it reduces the computational load, and by using max pooling (or grabbing the highest values only), it highlights the parts of the feature which most align with the given feature the filter is trying to extract. As well as, by max pooling, this reduces overfitting (since we only look at general features by condensing rather than at outliers). The idea with all of these layers is to create virtually many “feature” detectors which can derive features while reducing computational complexity, noise, and overfitting. The idea behind the convolutional neural network will be to have the convolutional and pooling layer interleaved, so that the convolutional layer uses convolution to find the filter and the pooling layer uses pooling to effectively “highlight” the activation it finds and reducing the size of the input before it reaches the inner layers (where often you’ll be looking at smaller parts of an image). In total, this layout should allow us to find, compute, and return an array of features which can therefore be clustered on by a clustering algorithm such as K-Means or Fuzzy C-Means (as will be discussed below). 
\\
\indent Given the features from the CNN as an array of features {f1, …, fn}, the Fuzzy C-Means algorithm will look at these as a singular datapoint of dimension (or size) n. The main idea behind the FCM algorithm is given all the data points and a fuzzifier (a number denoting how hard or soft the clustering is), to compute suitable centers of groups of videos/video frames based on a common feature or group of features that are similar between them. The output will be the positions of the centers of the centroids, and a corresponding table of the images and their corresponding clusters denoted by a number j. To compute these centers, we’ll use the Fuzzy C-Means algorithm with the following pseudocode:
\begin {algorithm}
\caption {Fuzzy C Means} \label {algo}
\begin {algorithmic}[1]
	\State $f \gets  \, $4
%	\State $data_points \gets [f_1, f_2, …, f_n]
%	\State $centroids \gets [[0.1, 0.4, …, 0.05], …, k] 
	\State $notDone \gets  \, $False
	\While {notDone}:
		\State Calculate new centroid positions based on membership values of data points
		\State Recalculate the new membership values based on these new centroid positions compared to old ones
		\State Calculate diff to be the difference between the original centroid positions and new centroid positions
		%\If $diff <= \epsilon$
		%	\break
	
	\EndWhile
	(Optional) Decide on a primary cluster for each datapoint depending on the largest membership it has
	
	 centroid positions datapoint and corresponding clusters
\end{algorithmic}
\end{algorithm}
To determine the new centroid positions, it involves calculating the Euclidean distance between the datapoint and a given cluster. 
\\
\indent 
After these clusters have been computed, now the algorithm is set and ready to be tested. Placing a given image through the CNN gives us a relevant datapoint. Using this datapoint, we can determine initially what basket to check, as well as other viable options in order of membership value. Based on the data points distance to other data points, these corresponding images and video IDs will be returned first (since they are most similar in terms of visual characteristics). The progression of “relevance” will be determined by if the primary clusters are the same or not and then the corresponding distance to that datapoint. By computing this, we’ll get a list of videoIDs and their corresponding relevance depending on these factors. Much like Google’s search algorithm, this will mitigate the amount of time it takes for the user to find a relevant video.

\section{Evaluation}

\indent Not only do the evaluation of clustering algorithms allow us to test the cluster for the features/classes it groups upon, but it allows the developer (or myself) to identify and eliminate outliers, compare to human judgments, and understand the makeup of the data itself (Source). Within the context of this COMPS Project, evaluating the strengths and weaknesses of Fuzzy C-Means and K-Means clustering algorithms allowed me to introduce the idea of utilizing a center.
\\
\indent
To evaluate an image segmentation algorithm, it requires that we assess the strength of the clustering algorithm based on the meaningful classes that it generates. In other words, it should have clusters (even if they’re unlabeled), that resemble characteristics or blobs of image data that a human may draw out as meaningful. In our previous example, the Corgi in the tree, while clustered, was clustered into several different parts compared to just the Corgi in the white background. It’s clear that pixels do not serve as an identification for features, as colors can be shared by different parts or different objects within the image (in this case, the Corgi and the tree). There are mainly two parts I propose to evaluate a given clustering algorithm: the number of clusters and the strength of these clusters (i.e. how well can it cluster pixels of an object together).
\\
\indent
The number of clusters is a defining characteristic in clustering algorithms, but unfortunately a tricky measurement. Therefore, researched and defined methods such as the Elbow Method may provide a way to determine the number of clusters for a given amount of data. Fuzzy C-Means and K-Means are both computationally inefficient at O(n) and O(knt) respectively (Source), and therefore utilizing this can save on computational costs and determine how many clusters are needed to have enough variation in one computation. The idea is that the “elbow” or “knee” created by the graph acts as a point “where diminishing returns are no longer worth the additional cost” (Wikipedia). As with the examples, the respective “elbow” resides at 3 for the corgi tree photo and at 2 (or 3), up to interpretation.
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3] {corgi_tree_elbow}
  \vspace{20px}
  \caption{The elbow method applied to the original corgi in tree image. Notice the "elbow" at 3 clusters.}
  \label{ctree:elbow}
\end{figure}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3] {corgi_white_elbow}
  \vspace{20px}
  \caption{The elbow method applied to the original corgi in white background image. Notice the "elbow" at 2 (or 4) clusters; this highlights how with less color density, less clusters are needed. As well as, this highlights an ambiguity issue with reading the results of the Elbow Method.}
  \label{cwhite:elbow}
\end{figure}
\\
\indent
If we want to evaluate the strength of these clusters, there are several methods, but I propose utilizing combinations of comparisons between the prediction and ground truth such as Intersection-over-Union and a Confusion Matrix as these are not only widely used representations (making them easy for others to evaluate themselves based on the results) (Mittal), but they are not mathematically difficult to represent nor compute. Not to mention, it’s very easy to compare as we’re simply comparing pixel-to-pixel for clusters (as we’ll know all the pixels and their clusters given our methods) as opposed to instance segmentation which requires identifying the objects/classes.
A confusion matrix consists of representing the accuracy, precision, and recall based on storing the true values (true positive/negative) and negative values (false positive/negative) in a 2x2 matrix (Source). By utilizing formulas for accuracy, precision, and recall, we can define how accurate versus precise our clustering may be in relation to human judgment (Source). Rather than having to ask human judges to define the accuracy or even the cluster for us, we can determine how well it predicts the ground truth in comparison to previously human-segmented images. In turn, our evaluation can determine how strong the clustering algorithm can group similar features (and therefore similar videos) together without having to ask for human judgment.
\\
\indent
	While the clustering algorithm may be solid for determining images of related visual similarity, the last question to ask is whether these clustering algorithms lead to related videos (of most importantly related content). As computers cannot evaluate the similarity between videos based on content, my plan is to ask others to test the program with test images of their own (and my own) and determine whether the related videos are helpful or not.









\printbibliography


\end{document}
